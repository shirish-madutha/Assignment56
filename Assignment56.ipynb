{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4db65d-56cb-465c-916c-5a37fa753298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. How does bagging reduce overfitting in decision trees? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Bagging reduces overfitting in decision trees by training multiple trees on different subsets of the training data. Since each tree sees only a subset, they are less likely to memorize noise or outliers in the data, which helps in creating more robust and generalizable models. The aggregation of these trees then reduces the overall variance of the model. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe2d2a-0497-40e7-9c25-901e5ebbe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. What are the advantages and disadvantages of using different types of base learners in bagging? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Advantages of using different types of base learners in bagging:\n",
    "\n",
    "Diversity: Different base learners capture different aspects of the data, increasing the diversity of the ensemble.\n",
    "Robustness: Combining models with different strengths can lead to a more robust overall model.\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Using diverse base learners may increase the complexity of the ensemble, making it harder to interpret.\n",
    "Training time: Training diverse base learners might require more computational resources. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3425c1-90d6-42e6-8cf3-561f64ec4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The choice of base learner in bagging can affect the bias-variance tradeoff. Using more complex base learners (e.g., deep decision trees) might lead to lower bias but higher variance. On the other hand, simpler base learners (e.g., shallow decision trees) might have higher bias but lower variance. Bagging tends to reduce variance, making it particularly useful when the base learners have high variance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa31b9b-6667-43c1-9789-4d8bced3e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Yes, bagging can be used for both classification and regression tasks. In classification, the ensemble's final prediction is often determined by a majority vote, while in regression, it is determined by averaging the predictions of individual models. The underlying principle of aggregating predictions from multiple models remains the same in both cases. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca554c-8332-439f-987e-02e2d18abfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?  \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The ensemble size in bagging refers to the number of base learners included in the ensemble. As a general guideline, increasing the ensemble size tends to improve performance up to a certain point. However, there is a diminishing return, and adding too many models may not significantly improve performance while increasing computational costs. The optimal ensemble size depends on the specific problem and dataset and may require experimentation to find the right balance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415aee8-3d62-4675-afdd-2634ccb3a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q6. Can you provide an example of a real-world application of bagging in machine learning? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Real-world application of bagging in machine learning:\n",
    "\n",
    "Random Forest for Image Classification: Bagging is commonly applied in image classification tasks using Random Forests. Each decision tree in the ensemble is trained on a different subset of images, capturing various features and patterns. The final ensemble provides a robust and accurate classification for diverse images, reducing overfitting and improving generalization. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
